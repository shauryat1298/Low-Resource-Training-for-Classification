{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import vocab\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"C:/Users/shaur/Desktop/Research/low_resource_training/pos_tagging\")\n",
    "artifacts_path = os.path.join(base_path, \"artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "tagged_sentences = brown_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "\n",
    "    for entity in sentence:         \n",
    "        X.append(entity[0].lower())  # entity[0] contains the word\n",
    "        Y.append(entity[1])  # entity[1] contains corresponding tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map targets to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['CONJ', 'DET', 'PRT', 'VERB', 'ADV', 'X', 'ADJ', 'NUM', 'PRON', 'NOUN', '.', 'ADP']\n",
    "\n",
    "id2label = {}\n",
    "for id_, label_ in enumerate(class_names):\n",
    "    id2label[str(id_)] = label_\n",
    "\n",
    "label2id = {}\n",
    "for id_, label_ in enumerate(class_names):\n",
    "    label2id[label_] = id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_id = []\n",
    "for i in range(len(Y)):\n",
    "    Y_id.append(label2id[Y[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, Y_id, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        texts = self.X[idx]\n",
    "        labels = self.y[idx]\n",
    "        sample = (labels, texts)\n",
    "        return sample\n",
    "    \n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataset, min_freq=1):\n",
    "\n",
    "    counter_word = Counter()\n",
    "    for word in dataset: \n",
    "        counter_word.update([word])\n",
    "    my_vocab_word = vocab(counter_word, min_freq=min_freq)\n",
    "    my_vocab_word.insert_token('<unk>', 0)\n",
    "    my_vocab_word.set_default_index(0)\n",
    "\n",
    "    counter_character = Counter()\n",
    "    for word in dataset:\n",
    "        for i in range(0, len(word)):\n",
    "            counter_character.update([word[i]])\n",
    "    my_vocab_character = vocab(counter_character, min_freq=min_freq)\n",
    "    my_vocab_character.insert_token('<unk>', 0)\n",
    "    my_vocab_character.set_default_index(0)\n",
    "\n",
    "    counter_bigram = Counter()\n",
    "    for word in dataset:\n",
    "        for i in range(0, len(word)):\n",
    "            if i<len(word)-1:\n",
    "                counter_bigram.update([word[i:i+2]])\n",
    "    my_vocab_bigram = vocab(counter_bigram, min_freq=min_freq)\n",
    "    my_vocab_bigram.insert_token('<unk>', 0)\n",
    "    my_vocab_bigram.set_default_index(0)\n",
    "\n",
    "    counter_trigram = Counter()\n",
    "    for word in dataset:\n",
    "        for i in range(0, len(word)):\n",
    "            if i<len(word)-2:\n",
    "                counter_trigram.update([word[i:i+3]])\n",
    "    my_vocab_trigram = vocab(counter_trigram, min_freq=min_freq)\n",
    "    my_vocab_trigram.insert_token('<unk>', 0)\n",
    "    my_vocab_trigram.set_default_index(0)\n",
    "\n",
    "    return my_vocab_word, my_vocab_character, my_vocab_bigram, my_vocab_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_word, vocab_character, vocab_bigram, vocab_trigram = get_vocab(X, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate Function for Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    \"\"\"Converts text to a list of word, character, bigram, and trigram indices using vocabulary dictionaries.\"\"\"\n",
    "    \n",
    "    word_indices_list, ch_indices_lists, bigram_indices_lists, trigram_indices_lists = 0, [], [], []\n",
    "\n",
    "    for word in x:\n",
    "        # Convert word to index using vocab_word\n",
    "        word_indices_list= vocab_word([word])[0]  \n",
    "        \n",
    "        # ch_indices_list, bigram_indices_list, trigram_indices_list = [], [], []\n",
    "        \n",
    "        # Convert characters to indices\n",
    "        for i in range(len(word)):\n",
    "            ch_indices_lists.append(vocab_character([word[i]])[0])  # Using vocab_char for individual characters\n",
    "            \n",
    "            # Convert bigrams to indices if applicable\n",
    "            if i < len(word) - 1:\n",
    "                bigram = word[i:i+2]\n",
    "                bigram_indices_lists.append(vocab_bigram([bigram])[0])  # Using vocab_bigram\n",
    "            \n",
    "            # Convert trigrams to indices if applicable\n",
    "            if i < len(word) - 2:\n",
    "                trigram = word[i:i+3]\n",
    "                trigram_indices_lists.append(vocab_trigram([trigram])[0])  # Using vocab_trigram\n",
    "        \n",
    "        # ch_indices_lists.append(ch_indices_list)\n",
    "        # bigram_indices_lists.append(bigram_indices_list)\n",
    "        # trigram_indices_lists.append(trigram_indices_list)\n",
    "    \n",
    "    return word_indices_list, ch_indices_lists, bigram_indices_lists, trigram_indices_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerDataLoader:\n",
    "    def __init__(self, tokenizer, batch_size=32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.data = []\n",
    "\n",
    "    def add_data(self, texts):\n",
    "        for text in texts:\n",
    "            self.data.append(self.tokenizer([text]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def get_batches(self):\n",
    "        for i in range(0, len(self.data), self.batch_size):\n",
    "            yield self.data[i:i + self.batch_size]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        merged_embeddings = {\n",
    "            'word': [],\n",
    "            'character': [],\n",
    "            'bigram': [],\n",
    "            'trigram': []\n",
    "        }\n",
    "        for item in batch:\n",
    "            word_indices, ch_indices, bigram_indices, trigram_indices = item\n",
    "            merged_embeddings['word'].append(word_indices)\n",
    "            merged_embeddings['character'].append(ch_indices)\n",
    "            merged_embeddings['bigram'].append(bigram_indices)\n",
    "            merged_embeddings['trigram'].append(trigram_indices)\n",
    "        return merged_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of TokenizerDataLoader\n",
    "dataloader = TokenizerDataLoader(tokenizer, batch_size=32)\n",
    "\n",
    "# Add data\n",
    "texts = [\"text\", \"example\"]\n",
    "dataloader.add_data(texts)\n",
    "\n",
    "# Get batches\n",
    "for batch in dataloader.get_batches():\n",
    "    # Process the batch\n",
    "    processed_batch = dataloader.collate_fn(batch)\n",
    "    # Use the processed batch for training or further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': [1420, 3037],\n",
       " 'character': [[1, 3, 26, 1], [3, 26, 13, 21, 20, 6, 3]],\n",
       " 'bigram': [[80, 89, 235], [89, 334, 186, 196, 76, 52]],\n",
       " 'trigram': [[968, 473], [969, 2449, 1062, 452, 453]]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 word_vocab_size, \n",
    "                 ch_vocab_size, \n",
    "                 bigram_vocab_size, \n",
    "                 trigram_vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim1, \n",
    "                 hidden_dim2, \n",
    "                 drop_prob1, \n",
    "                 drop_prob2, \n",
    "                 num_outputs):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_word = nn.Embedding(word_vocab_size, embedding_dim)\n",
    "        self.embedding_ch = nn.Embedding(ch_vocab_size, embedding_dim)\n",
    "        self.embedding_bigram = nn.Embedding(bigram_vocab_size, embedding_dim)\n",
    "        self.embedding_trigram = nn.Embedding(trigram_vocab_size, embedding_dim)\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim*4, hidden_dim1)\n",
    "        # Batch normalization for first linear layer\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features=hidden_dim1)\n",
    "        # Dropout for first linear layer\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob1)\n",
    "\n",
    "        # Second Linear layer\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        # Batch normalization for second linear layer\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_features=hidden_dim2)\n",
    "        # Dropout for second linear layer\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob2)\n",
    "\n",
    "        # Final Linear layer\n",
    "        self.linear3 = nn.Linear(hidden_dim2, num_outputs)\n",
    "\n",
    "    def forward(self, input_tuple):\n",
    "\n",
    "        print(input_tuple)\n",
    "\n",
    "        indices_word, indices_ch, indices_bigram, indices_trigram = input_tuple['word'], input_tuple['character'], input_tuple['bigram'], input_tuple['trigram']\n",
    "\n",
    "        emb_word, emb_ch, emb_bigram, emb_trigram = self.embedding_word(indices_word), self.embedding_ch(indices_ch), self.embedding_bigram(indices_bigram), self.embedding_trigram(indices_trigram)\n",
    "        x = torch.cat((emb_word, emb_ch, emb_bigram, emb_trigram), dim=-1)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_nested_list(list):\n",
    "    list = [torch.tensor(sublist) for sublist in list]\n",
    "    return rnn_utils.pad_sequence(list, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1420, 3037], device='cuda:0'), tensor([[ 1,  3, 26,  1,  0,  0,  0],\n",
      "        [ 3, 26, 13, 21, 20,  6,  3]], device='cuda:0'), tensor([[ 80,  89, 235,   0,   0,   0],\n",
      "        [ 89, 334, 186, 196,  76,  52]], device='cuda:0'), tensor([[ 968,  473,    0,    0,    0],\n",
      "        [ 969, 2449, 1062,  452,  453]], device='cuda:0'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\research_env\\lib\\site-packages\\torchinfo\\torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39mx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\research_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\research_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[1;32mIn[91], line 41\u001b[0m, in \u001b[0;36mSimpleMLP.forward\u001b[1;34m(self, input_tuple)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_tuple)\n\u001b[1;32m---> 41\u001b[0m indices_word, indices_ch, indices_bigram, indices_trigram \u001b[38;5;241m=\u001b[39m \u001b[43minput_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, input_tuple[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacter\u001b[39m\u001b[38;5;124m'\u001b[39m], input_tuple[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigram\u001b[39m\u001b[38;5;124m'\u001b[39m], input_tuple[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrigram\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     43\u001b[0m emb_word, emb_ch, emb_bigram, emb_trigram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_word(indices_word), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ch(indices_ch), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_bigram(indices_bigram), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_trigram(indices_trigram)\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m indices_trigram \u001b[38;5;241m=\u001b[39m pad_nested_list(indices_trigram)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Generate summary\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_bigram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_trigram\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\research_env\\lib\\site-packages\\torchinfo\\torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m validate_user_params(\n\u001b[0;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[0;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[0;32m    222\u001b[0m )\n\u001b[1;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m forward_pass(\n\u001b[0;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[0;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[0;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[0;32m    229\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\research_env\\lib\\site-packages\\torchinfo\\torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the sequential model\n",
    "# this will invoke the __init__() function of the model\n",
    "model = SimpleMLP(word_vocab_size = 7, \n",
    "                 ch_vocab_size = 7, \n",
    "                 bigram_vocab_size = 7, \n",
    "                 trigram_vocab_size = 7, \n",
    "                 embedding_dim = 20, \n",
    "                 hidden_dim1 = 10, \n",
    "                 hidden_dim2 = 5, \n",
    "                 drop_prob1 = 0.5, \n",
    "                 drop_prob2 = 0.5, \n",
    "                 num_outputs = 2)\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate some dummy input data and offsets, and move them to the device\n",
    "indices_word, indices_ch, indices_bigram, indices_trigram = processed_batch['word'], processed_batch['character'], processed_batch['bigram'], processed_batch['trigram']\n",
    "indices_word = torch.tensor(indices_word, dtype=torch.int64)\n",
    "indices_ch = pad_nested_list(indices_ch)\n",
    "indices_bigram = pad_nested_list(indices_bigram)\n",
    "indices_trigram = pad_nested_list(indices_trigram)\n",
    "\n",
    "# Generate summary\n",
    "summary(model, input_data=[(indices_word, indices_ch, indices_bigram, indices_trigram)], device=device, depth =10, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 1,  3, 26,  1]), tensor([ 3, 26, 13, 21, 20,  6,  3])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "nested_list = [[1, 3, 26, 1], [3, 26, 13, 21, 20, 6, 3]]\n",
    "tensor_list = [torch.tensor(sublist) for sublist in nested_list]\n",
    "\n",
    "print(tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4889,  1.3869],\n",
      "        [-1.6105,  0.5258],\n",
      "        [ 1.0008, -1.2242],\n",
      "        [-0.2340,  0.5366]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model((word_ind, ch_ind, bigram_ind, trigram_ind))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
