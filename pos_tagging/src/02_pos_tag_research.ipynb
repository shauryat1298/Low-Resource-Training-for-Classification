{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaur\\anaconda3\\envs\\research_env\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\shaur\\anaconda3\\envs\\research_env\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import vocab\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"C:/Users/shaur/Desktop/Research/low_resource_training/pos_tagging\")\n",
    "artifacts_path = os.path.join(base_path, \"artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "tagged_sentences = brown_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "\n",
    "    for entity in sentence:         \n",
    "        X.append(entity[0].lower())  # entity[0] contains the word\n",
    "        Y.append(entity[1])  # entity[1] contains corresponding tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataset, min_freq=1):\n",
    "\n",
    "    counter_word = Counter()\n",
    "    for word in dataset: \n",
    "        counter_word.update([word])\n",
    "    my_vocab_word = vocab(counter_word, min_freq=min_freq)\n",
    "    my_vocab_word.insert_token('<unk>', 0)\n",
    "    my_vocab_word.set_default_index(0)\n",
    "\n",
    "    counter_character = Counter()\n",
    "    for word in dataset:\n",
    "        for i in range(0, len(word)):\n",
    "            counter_character.update([word[i]])\n",
    "    my_vocab_character = vocab(counter_character, min_freq=min_freq)\n",
    "    my_vocab_character.insert_token('<unk>', 0)\n",
    "    my_vocab_character.set_default_index(0)\n",
    "\n",
    "    counter_bigram = Counter()\n",
    "    for word in dataset:\n",
    "        for i in range(0, len(word)):\n",
    "            if i<len(word)-1:\n",
    "                counter_bigram.update([word[i:i+2]])\n",
    "    my_vocab_bigram = vocab(counter_bigram, min_freq=min_freq)\n",
    "    my_vocab_bigram.insert_token('<unk>', 0)\n",
    "    my_vocab_bigram.set_default_index(0)\n",
    "\n",
    "    counter_trigram = Counter()\n",
    "    for word in dataset:\n",
    "        for i in range(0, len(word)):\n",
    "            if i<len(word)-2:\n",
    "                counter_trigram.update([word[i:i+3]])\n",
    "    my_vocab_trigram = vocab(counter_trigram, min_freq=min_freq)\n",
    "    my_vocab_trigram.insert_token('<unk>', 0)\n",
    "    my_vocab_trigram.set_default_index(0)\n",
    "\n",
    "    return my_vocab_word, my_vocab_character, my_vocab_bigram, my_vocab_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_word, vocab_character, vocab_bigram, vocab_trigram = get_vocab(X, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate Function for Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    \"\"\"Converts text to a list of indices using a vocabulary dictionary\"\"\"\n",
    "    word_indices_list, ch_indices_lists, bigram_indices_lists, trigram_indices_lists = [], [], [], []\n",
    "\n",
    "    for word in x:\n",
    "        word_indices_list.append(vocab_word(word))\n",
    "        ch_indices_list, bigram_indices_list, trigram_indices_list = [], [], []\n",
    "        for i in range(0, len(word)):\n",
    "            ch_indices_list.append(word[i])\n",
    "            if i<len(word)-1: bigram_indices_list.append(word[i:i+2])\n",
    "            if i<len(word)-2: trigram_indices_list.append(word[i:i+3])\n",
    "\n",
    "        ch_indices_lists.append(ch_indices_list)\n",
    "        bigram_indices_lists.append(bigram_indices_list)\n",
    "        trigram_indices_lists.append(trigram_indices_list)\n",
    "    \n",
    "    return word_indices_list, ch_indices_lists, bigram_indices_lists, trigram_indices_lists\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_emb(batch):\n",
    "\n",
    "    labels, texts = zip(*batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deine Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 word_vocab_size, \n",
    "                 ch_vocab_size, \n",
    "                 bigram_vocab_size, \n",
    "                 trigram_vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim1, \n",
    "                 hidden_dim2, \n",
    "                 drop_prob1, \n",
    "                 drop_prob2, \n",
    "                 num_outputs):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_word = nn.Embedding(word_vocab_size, embedding_dim)\n",
    "        self.embedding_ch = nn.Embedding(ch_vocab_size, embedding_dim)\n",
    "        self.embedding_bigram = nn.Embedding(bigram_vocab_size, embedding_dim)\n",
    "        self.embedding_trigram = nn.Embedding(trigram_vocab_size, embedding_dim)\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim*4, hidden_dim1)\n",
    "        # Batch normalization for first linear layer\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features=hidden_dim1)\n",
    "        # Dropout for first linear layer\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob1)\n",
    "\n",
    "        # Second Linear layer\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        # Batch normalization for second linear layer\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_features=hidden_dim2)\n",
    "        # Dropout for second linear layer\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob2)\n",
    "\n",
    "        # Final Linear layer\n",
    "        self.linear3 = nn.Linear(hidden_dim2, num_outputs)\n",
    "\n",
    "    def forward(self, input_tuple):\n",
    "\n",
    "        indices_word, indices_ch, indices_bigram, indices_trigram = input_tuple\n",
    "\n",
    "        emb_word, emb_ch, emb_bigram, emb_trigram = self.embedding_word(indices_word), self.embedding_ch(indices_ch), self.embedding_bigram(indices_bigram), self.embedding_trigram(indices_trigram)\n",
    "        x = torch.cat((emb_word, emb_ch, emb_bigram, emb_trigram), dim=-1)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleMLP                                [5, 2]                    --\n",
       "├─Embedding: 1-1                         [5, 20]                   140\n",
       "├─Embedding: 1-2                         [5, 20]                   140\n",
       "├─Embedding: 1-3                         [5, 20]                   140\n",
       "├─Embedding: 1-4                         [5, 20]                   140\n",
       "├─Linear: 1-5                            [5, 10]                   810\n",
       "├─BatchNorm1d: 1-6                       [5, 10]                   20\n",
       "├─Dropout: 1-7                           [5, 10]                   --\n",
       "├─Linear: 1-8                            [5, 5]                    55\n",
       "├─BatchNorm1d: 1-9                       [5, 5]                    10\n",
       "├─Dropout: 1-10                          [5, 5]                    --\n",
       "├─Linear: 1-11                           [5, 2]                    12\n",
       "==========================================================================================\n",
       "Total params: 1,467\n",
       "Trainable params: 1,467\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the sequential model\n",
    "# this will invoke the __init__() function of the model\n",
    "model = SimpleMLP(word_vocab_size = 7, \n",
    "                 ch_vocab_size = 7, \n",
    "                 bigram_vocab_size = 7, \n",
    "                 trigram_vocab_size = 7, \n",
    "                 embedding_dim = 20, \n",
    "                 hidden_dim1 = 10, \n",
    "                 hidden_dim2 = 5, \n",
    "                 drop_prob1 = 0.5, \n",
    "                 drop_prob2 = 0.5, \n",
    "                 num_outputs = 2)\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate some dummy input data and offsets, and move them to the device\n",
    "word_ind = torch.tensor([1, 2, 4, 5, 4], dtype = torch.int32).to(device)\n",
    "ch_ind = torch.tensor([1, 2, 4, 5, 4], dtype = torch.int32).to(device)\n",
    "bigram_ind = torch.tensor([1, 2, 4, 5, 4], dtype = torch.int32).to(device)\n",
    "trigram_ind = torch.tensor([1, 2, 4, 5, 4], dtype = torch.int32).to(device)\n",
    "\n",
    "# Generate summary\n",
    "summary(model, input_data=[(word_ind, ch_ind, bigram_ind, trigram_ind)], device=device, depth =10, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4844,  0.3000],\n",
      "        [ 0.3034,  0.4267],\n",
      "        [ 0.3154,  0.0952],\n",
      "        [-0.0282,  0.1706],\n",
      "        [ 0.5052,  0.1715]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model((word_ind, ch_ind, bigram_ind, trigram_ind))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
